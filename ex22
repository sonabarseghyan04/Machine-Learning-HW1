import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_moons
from sklearn import datasets
from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, GridSearchCV
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.datasets import make_circles


df = pd.read_csv('/Users/sonabarseghyan/Desktop/House price prediction data.csv')

#droping NA values in case there are some
df = df.dropna() 

df_encoded = pd.get_dummies(df, drop_first=True) #get_dummies function does hot encoding on categorical values
#whenever it finds them. For example in our csv, all the columns having yes/no are categorical. And also we are 
#dropping the first value of the result to avoid multicollinearity: in other words, if we know all the values of
#n-1 categories, whether they are 1 or 0, we can infer what is the value of nth category(omitting the dummy variable).
 
X = df_encoded.drop('price', axis=1) #setting the features(X) excluding the dependent variable price
y = df_encoded['price']  #setting price as dependent variable

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X) #feature scaling the numerical columns

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.15,  random_state=42)

# Train the Multiple Linear Regression model
regression_model = LinearRegression()
regression_model.fit(X_train, y_train)

y_pred = regression_model.predict(X_test)

r2 = r2_score(y_test, y_pred)
n = X_test.shape[0]  # Getting number of samples
p = X_test.shape[1]  # Getting number of features
adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)

print(f"R²: {r2:.4f}")
print(f"Adjusted R²: {adjusted_r2:.4f}")

